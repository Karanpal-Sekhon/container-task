{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingFace Model Inference Demo\n",
    "\n",
    "This notebook demonstrates parallel requests to our containerized T5-small model inference server.\n",
    "\n",
    "## Why T5-small?\n",
    "\n",
    "T5-small was chosen for this demonstration because:\n",
    "- **Lightweight**: 60M parameters, fast inference suitable for demo\n",
    "- **Versatile**: Text-to-text format handles multiple NLP tasks  \n",
    "- **Production-ready**: Well-tested HuggingFace model with consistent performance\n",
    "- **Resource efficient**: Works well in containerized environments without GPU requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mconcurrent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfutures\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datetime\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import concurrent.futures\n",
    "from datetime import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Server configuration\n",
    "SERVER_URL = \"http://localhost\"  # nginx proxy endpoint\n",
    "GENERATE_ENDPOINT = f\"{SERVER_URL}/generate\"\n",
    "\n",
    "print(f\"Server endpoint: {GENERATE_ENDPOINT}\")\n",
    "\n",
    "# Test server health first\n",
    "health_response = requests.get(f\"{SERVER_URL}/health\")\n",
    "print(f\"Health check: {health_response.status_code} - {health_response.json()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test single request\n",
    "test_request = {\n",
    "    \"text\": \"translate English to German: Hello world\",\n",
    "    \"max_length\": 50,\n",
    "    \"temperature\": 1.0,\n",
    "    \"num_beams\": 4\n",
    "}\n",
    "\n",
    "print(\"Testing single request...\")\n",
    "start_time = time.time()\n",
    "response = requests.post(GENERATE_ENDPOINT, json=test_request)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Status: {response.status_code}\")\n",
    "print(f\"Request time: {end_time - start_time:.3f}s\")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(f\"Input: {result['input_text']}\")\n",
    "    print(f\"Output: {result['generated_text']}\")\n",
    "    print(f\"Model time: {result['generation_time_seconds']:.3f}s\")\n",
    "else:\n",
    "    print(f\"Error: {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_inference_request(request_data, request_id):\n",
    "    \"\"\"Make a single inference request and return timing data\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(GENERATE_ENDPOINT, json=request_data, timeout=30)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        result = {\n",
    "            'request_id': request_id,\n",
    "            'status_code': response.status_code,\n",
    "            'total_time': end_time - start_time,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            json_response = response.json()\n",
    "            result['input_text'] = json_response['input_text']\n",
    "            result['generated_text'] = json_response['generated_text']\n",
    "            result['model_time'] = json_response['generation_time_seconds']\n",
    "            result['success'] = True\n",
    "        else:\n",
    "            result['error'] = response.text\n",
    "            result['success'] = False\n",
    "            \n",
    "    except Exception as e:\n",
    "        end_time = time.time()\n",
    "        result = {\n",
    "            'request_id': request_id,\n",
    "            'status_code': 0,\n",
    "            'total_time': end_time - start_time,\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'error': str(e),\n",
    "            'success': False\n",
    "        }\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"Request function defined successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample requests demonstrating different NLP tasks\n",
    "sample_requests = [\n",
    "    # Translation tasks\n",
    "    {\"text\": \"translate English to German: Hello world\", \"max_length\": 50},\n",
    "    {\"text\": \"translate English to French: Good morning\", \"max_length\": 50},\n",
    "    {\"text\": \"translate English to Spanish: How are you today?\", \"max_length\": 50},\n",
    "    \n",
    "    # Summarization tasks\n",
    "    {\"text\": \"summarize: The weather today is beautiful with clear skies and sunshine. Temperature is perfect for outdoor activities.\", \"max_length\": 30},\n",
    "    {\"text\": \"summarize: Machine learning is a subset of artificial intelligence that enables computers to learn without explicit programming.\", \"max_length\": 25},\n",
    "    \n",
    "    # Question answering / extraction tasks\n",
    "    {\"text\": \"question: What is the capital of France? context: Paris is the capital and largest city of France.\", \"max_length\": 20},\n",
    "    {\"text\": \"question: Who invented the telephone? context: Alexander Graham Bell invented the telephone in 1876.\", \"max_length\": 20},\n",
    "    \n",
    "    # Text completion tasks\n",
    "    {\"text\": \"complete: The benefits of exercise include\", \"max_length\": 40},\n",
    "]\n",
    "\n",
    "print(f\"Prepared {len(sample_requests)} sample requests across different NLP tasks:\")\n",
    "print(\"\\nTranslation tasks:\")\n",
    "for i, req in enumerate(sample_requests[:3]):\n",
    "    print(f\"  {i+1}. {req['text']}\")\n",
    "\n",
    "print(\"\\nSummarization tasks:\")\n",
    "for i, req in enumerate(sample_requests[3:5]):\n",
    "    print(f\"  {i+4}. {req['text']}\")\n",
    "\n",
    "print(\"\\nQuestion answering tasks:\")\n",
    "for i, req in enumerate(sample_requests[5:7]):\n",
    "    print(f\"  {i+6}. {req['text']}\")\n",
    "    \n",
    "print(\"\\nText completion tasks:\")\n",
    "for i, req in enumerate(sample_requests[7:]):\n",
    "    print(f\"  {i+8}. {req['text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute parallel requests\n",
    "print(\"Starting parallel inference requests...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Use ThreadPoolExecutor to make concurrent requests\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    # Submit all requests\n",
    "    futures = [\n",
    "        executor.submit(make_inference_request, request, i+1) \n",
    "        for i, request in enumerate(sample_requests)\n",
    "    ]\n",
    "    \n",
    "    # Collect results as they complete\n",
    "    results = []\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        result = future.result()\n",
    "        results.append(result)\n",
    "        print(f\"Request {result['request_id']} completed: {result['success']}\")\n",
    "\n",
    "end_time = time.time()\n",
    "total_parallel_time = end_time - start_time\n",
    "\n",
    "print(f\"\\nAll {len(sample_requests)} requests completed in {total_parallel_time:.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort results by request_id for consistent display\n",
    "results.sort(key=lambda x: x['request_id'])\n",
    "\n",
    "print(\"=== PARALLEL REQUEST RESULTS ===\\n\")\n",
    "\n",
    "successful_requests = 0\n",
    "for result in results:\n",
    "    print(f\"Request {result['request_id']}:\")\n",
    "    print(f\"  Status: {'✅ SUCCESS' if result['success'] else '❌ FAILED'}\")\n",
    "    print(f\"  Total Time: {result['total_time']:.3f}s\")\n",
    "    \n",
    "    if result['success']:\n",
    "        successful_requests += 1\n",
    "        print(f\"  Input: {result['input_text']}\")\n",
    "        print(f\"  Output: {result['generated_text']}\")\n",
    "        print(f\"  Model Time: {result['model_time']:.3f}s\")\n",
    "    else:\n",
    "        print(f\"  Error: {result.get('error', 'Unknown error')}\")\n",
    "    print()\n",
    "\n",
    "print(f\"Success Rate: {successful_requests}/{len(results)} ({100*successful_requests/len(results):.1f}%)\")\n",
    "print(f\"Total Parallel Execution Time: {total_parallel_time:.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame for performance analysis\n",
    "if successful_requests > 0:\n",
    "    successful_results = [r for r in results if r['success']]\n",
    "    \n",
    "    df = pd.DataFrame(successful_results)\n",
    "    \n",
    "    print(\"=== PERFORMANCE ANALYSIS ===\\n\")\n",
    "    print(\"Timing Statistics:\")\n",
    "    print(f\"  Average total time: {df['total_time'].mean():.3f}s\")\n",
    "    print(f\"  Average model time: {df['model_time'].mean():.3f}s\")\n",
    "    print(f\"  Min total time: {df['total_time'].min():.3f}s\")\n",
    "    print(f\"  Max total time: {df['total_time'].max():.3f}s\")\n",
    "    \n",
    "    # Calculate theoretical sequential time\n",
    "    sequential_time = df['total_time'].sum()\n",
    "    speedup = sequential_time / total_parallel_time\n",
    "    \n",
    "    print(f\"\\nParallelization Benefits:\")\n",
    "    print(f\"  Sequential execution would take: {sequential_time:.3f}s\")\n",
    "    print(f\"  Parallel execution took: {total_parallel_time:.3f}s\") \n",
    "    print(f\"  Speedup factor: {speedup:.2f}x\")\n",
    "    \n",
    "    print(f\"\\nDetailed Results Table:\")\n",
    "    display_df = df[['request_id', 'total_time', 'model_time', 'input_text', 'generated_text']].copy()\n",
    "    display_df['input_text'] = display_df['input_text'].str[:50] + '...'\n",
    "    display_df['generated_text'] = display_df['generated_text'].str[:50] + '...'\n",
    "    print(display_df.to_string(index=False))\n",
    "else:\n",
    "    print(\"No successful requests to analyze\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo Summary\n",
    "\n",
    "This notebook successfully demonstrated:\n",
    "\n",
    "1. **Multiple NLP Tasks**: T5-small handled translation, summarization, question answering, and text completion\n",
    "2. **Parallel Processing**: Concurrent requests were processed efficiently by the containerized server\n",
    "3. **Performance Benefits**: Parallel execution provided significant speedup over sequential processing\n",
    "4. **Production Readiness**: The server handled multiple concurrent requests reliably\n",
    "\n",
    "## Architecture Benefits\n",
    "\n",
    "- **nginx Load Balancer**: Distributes requests across multiple workers\n",
    "- **FastAPI Async**: Non-blocking request handling for better concurrency  \n",
    "- **Containerized Deployment**: Consistent, scalable deployment environment\n",
    "- **Health Monitoring**: Kubernetes-ready health checks for production deployment\n",
    "\n",
    "The demo proves the system can handle real-world parallel inference workloads effectively."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
