{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingFace Model Inference Demo\n",
    "\n",
    "This notebook demonstrates parallel requests to our containerized T5-small model inference server.\n",
    "\n",
    "## Why T5-small?\n",
    "\n",
    "T5-small was chosen for this demonstration because:\n",
    "- **Lightweight**: 60M parameters, fast inference suitable for demo\n",
    "- **Versatile**: Text-to-text format handles multiple NLP tasks  \n",
    "- **Production-ready**: Well-tested HuggingFace model with consistent performance\n",
    "- **Resource efficient**: Works well in containerized environments without GPU requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /home/kssekhon/.local/lib/python3.10/site-packages (2.2.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /home/kssekhon/.local/lib/python3.10/site-packages (from pandas) (2.2.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/kssekhon/.local/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/kssekhon/.local/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import concurrent.futures\n",
    "from datetime import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server endpoint: http://localhost/generate\n",
      "Health check: 200 - {'status': 'healthy', 'timestamp': '2025-06-04T04:08:04.235644', 'message': 'Server is running and accepting requests'}\n"
     ]
    }
   ],
   "source": [
    "# Server configuration\n",
    "SERVER_URL = \"http://localhost\"  # nginx proxy endpoint\n",
    "GENERATE_ENDPOINT = f\"{SERVER_URL}/generate\"\n",
    "\n",
    "print(f\"Server endpoint: {GENERATE_ENDPOINT}\")\n",
    "\n",
    "# Test server health first\n",
    "health_response = requests.get(f\"{SERVER_URL}/health\")\n",
    "print(f\"Health check: {health_response.status_code} - {health_response.json()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing single request...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: 200\n",
      "Request time: 0.262s\n",
      "Input: translate English to German: Hello world\n",
      "Output: Hallo Welt\n",
      "Model time: 0.253s\n"
     ]
    }
   ],
   "source": [
    "# Test single request\n",
    "test_request = {\n",
    "    \"text\": \"translate English to German: Hello world\",\n",
    "    \"max_length\": 50,\n",
    "    \"temperature\": 1.0,\n",
    "    \"num_beams\": 4\n",
    "}\n",
    "\n",
    "print(\"Testing single request...\")\n",
    "start_time = time.time()\n",
    "response = requests.post(GENERATE_ENDPOINT, json=test_request)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Status: {response.status_code}\")\n",
    "print(f\"Request time: {end_time - start_time:.3f}s\")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(f\"Input: {result['input_text']}\")\n",
    "    print(f\"Output: {result['generated_text']}\")\n",
    "    print(f\"Model time: {result['generation_time_seconds']:.3f}s\")\n",
    "else:\n",
    "    print(f\"Error: {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request function defined successfully\n"
     ]
    }
   ],
   "source": [
    "def make_inference_request(request_data, request_id):\n",
    "    \"\"\"Make a single inference request and return timing data\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(GENERATE_ENDPOINT, json=request_data, timeout=30)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        result = {\n",
    "            'request_id': request_id,\n",
    "            'status_code': response.status_code,\n",
    "            'total_time': end_time - start_time,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            json_response = response.json()\n",
    "            result['input_text'] = json_response['input_text']\n",
    "            result['generated_text'] = json_response['generated_text']\n",
    "            result['model_time'] = json_response['generation_time_seconds']\n",
    "            result['success'] = True\n",
    "        else:\n",
    "            result['error'] = response.text\n",
    "            result['success'] = False\n",
    "            \n",
    "    except Exception as e:\n",
    "        end_time = time.time()\n",
    "        result = {\n",
    "            'request_id': request_id,\n",
    "            'status_code': 0,\n",
    "            'total_time': end_time - start_time,\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'error': str(e),\n",
    "            'success': False\n",
    "        }\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"Request function defined successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 8 sample requests across different NLP tasks:\n",
      "\n",
      "Translation tasks:\n",
      "  1. translate English to German: Hello world\n",
      "  2. translate English to French: Good morning\n",
      "  3. translate English to Spanish: How are you today?\n",
      "\n",
      "Summarization tasks:\n",
      "  4. summarize: The weather today is beautiful with clear skies and sunshine. Temperature is perfect for outdoor activities.\n",
      "  5. summarize: Machine learning is a subset of artificial intelligence that enables computers to learn without explicit programming.\n",
      "\n",
      "Question answering tasks:\n",
      "  6. question: What is the capital of France? context: Paris is the capital and largest city of France.\n",
      "  7. question: Who invented the telephone? context: Alexander Graham Bell invented the telephone in 1876.\n",
      "\n",
      "Text completion tasks:\n",
      "  8. complete: The benefits of exercise include\n"
     ]
    }
   ],
   "source": [
    "# Sample requests demonstrating different NLP tasks\n",
    "sample_requests = [\n",
    "    # Translation tasks\n",
    "    {\"text\": \"translate English to German: Hello world\", \"max_length\": 50},\n",
    "    {\"text\": \"translate English to French: Good morning\", \"max_length\": 50},\n",
    "    {\"text\": \"translate English to Spanish: How are you today?\", \"max_length\": 50},\n",
    "    \n",
    "    # Summarization tasks\n",
    "    {\"text\": \"summarize: The weather today is beautiful with clear skies and sunshine. Temperature is perfect for outdoor activities.\", \"max_length\": 30},\n",
    "    {\"text\": \"summarize: Machine learning is a subset of artificial intelligence that enables computers to learn without explicit programming.\", \"max_length\": 25},\n",
    "    \n",
    "    # Question answering / extraction tasks\n",
    "    {\"text\": \"question: What is the capital of France? context: Paris is the capital and largest city of France.\", \"max_length\": 20},\n",
    "    {\"text\": \"question: Who invented the telephone? context: Alexander Graham Bell invented the telephone in 1876.\", \"max_length\": 20},\n",
    "    \n",
    "    # Text completion tasks\n",
    "    {\"text\": \"complete: The benefits of exercise include\", \"max_length\": 40},\n",
    "]\n",
    "\n",
    "print(f\"Prepared {len(sample_requests)} sample requests across different NLP tasks:\")\n",
    "print(\"\\nTranslation tasks:\")\n",
    "for i, req in enumerate(sample_requests[:3]):\n",
    "    print(f\"  {i+1}. {req['text']}\")\n",
    "\n",
    "print(\"\\nSummarization tasks:\")\n",
    "for i, req in enumerate(sample_requests[3:5]):\n",
    "    print(f\"  {i+4}. {req['text']}\")\n",
    "\n",
    "print(\"\\nQuestion answering tasks:\")\n",
    "for i, req in enumerate(sample_requests[5:7]):\n",
    "    print(f\"  {i+6}. {req['text']}\")\n",
    "    \n",
    "print(\"\\nText completion tasks:\")\n",
    "for i, req in enumerate(sample_requests[7:]):\n",
    "    print(f\"  {i+8}. {req['text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting parallel inference requests...\n",
      "Request 2 completed: True\n",
      "Request 1 completed: True\n",
      "Request 6 completed: True\n",
      "Request 8 completed: True\n",
      "Request 3 completed: True\n",
      "Request 7 completed: True\n",
      "Request 4 completed: True\n",
      "Request 5 completed: True\n",
      "\n",
      "All 8 requests completed in 2.963s\n"
     ]
    }
   ],
   "source": [
    "# Execute parallel requests\n",
    "print(\"Starting parallel inference requests...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Use ThreadPoolExecutor to make concurrent requests\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    # Submit all requests\n",
    "    futures = [\n",
    "        executor.submit(make_inference_request, request, i+1) \n",
    "        for i, request in enumerate(sample_requests)\n",
    "    ]\n",
    "    \n",
    "    # Collect results as they complete\n",
    "    results = []\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        result = future.result()\n",
    "        results.append(result)\n",
    "        print(f\"Request {result['request_id']} completed: {result['success']}\")\n",
    "\n",
    "end_time = time.time()\n",
    "total_parallel_time = end_time - start_time\n",
    "\n",
    "print(f\"\\nAll {len(sample_requests)} requests completed in {total_parallel_time:.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PARALLEL REQUEST RESULTS ===\n",
      "\n",
      "Request 1:\n",
      "  Status: ✅ SUCCESS\n",
      "  Total Time: 1.145s\n",
      "  Input: translate English to German: Hello world\n",
      "  Output: Hallo Welt\n",
      "  Model Time: 1.121s\n",
      "\n",
      "Request 2:\n",
      "  Status: ✅ SUCCESS\n",
      "  Total Time: 0.699s\n",
      "  Input: translate English to French: Good morning\n",
      "  Output: Bonjour\n",
      "  Model Time: 0.669s\n",
      "\n",
      "Request 3:\n",
      "  Status: ✅ SUCCESS\n",
      "  Total Time: 2.040s\n",
      "  Input: translate English to Spanish: How are you today?\n",
      "  Output: Wie sind Sie heute?\n",
      "  Model Time: 2.019s\n",
      "\n",
      "Request 4:\n",
      "  Status: ✅ SUCCESS\n",
      "  Total Time: 2.480s\n",
      "  Input: summarize: The weather today is beautiful with clear skies and sunshine. Temperature is perfect for outdoor activities.\n",
      "  Output: the weather today is beautiful with clear skies and sunshine.\n",
      "  Model Time: 2.457s\n",
      "\n",
      "Request 5:\n",
      "  Status: ✅ SUCCESS\n",
      "  Total Time: 2.949s\n",
      "  Input: summarize: Machine learning is a subset of artificial intelligence that enables computers to learn without explicit programming.\n",
      "  Output: machine learning is a subset of artificial intelligence that enables computers to learn without explicit programming.\n",
      "  Model Time: 2.939s\n",
      "\n",
      "Request 6:\n",
      "  Status: ✅ SUCCESS\n",
      "  Total Time: 0.655s\n",
      "  Input: question: What is the capital of France? context: Paris is the capital and largest city of France.\n",
      "  Output: Paris\n",
      "  Model Time: 0.631s\n",
      "\n",
      "Request 7:\n",
      "  Status: ✅ SUCCESS\n",
      "  Total Time: 0.981s\n",
      "  Input: question: Who invented the telephone? context: Alexander Graham Bell invented the telephone in 1876.\n",
      "  Output: Alexander Graham Bell\n",
      "  Model Time: 0.962s\n",
      "\n",
      "Request 8:\n",
      "  Status: ✅ SUCCESS\n",
      "  Total Time: 0.649s\n",
      "  Input: complete: The benefits of exercise include\n",
      "  Output: Complete:\n",
      "  Model Time: 0.635s\n",
      "\n",
      "Success Rate: 8/8 (100.0%)\n",
      "Total Parallel Execution Time: 2.963s\n"
     ]
    }
   ],
   "source": [
    "# Sort results by request_id for consistent display\n",
    "results.sort(key=lambda x: x['request_id'])\n",
    "\n",
    "print(\"=== PARALLEL REQUEST RESULTS ===\\n\")\n",
    "\n",
    "successful_requests = 0\n",
    "for result in results:\n",
    "    print(f\"Request {result['request_id']}:\")\n",
    "    print(f\"  Status: {'✅ SUCCESS' if result['success'] else '❌ FAILED'}\")\n",
    "    print(f\"  Total Time: {result['total_time']:.3f}s\")\n",
    "    \n",
    "    if result['success']:\n",
    "        successful_requests += 1\n",
    "        print(f\"  Input: {result['input_text']}\")\n",
    "        print(f\"  Output: {result['generated_text']}\")\n",
    "        print(f\"  Model Time: {result['model_time']:.3f}s\")\n",
    "    else:\n",
    "        print(f\"  Error: {result.get('error', 'Unknown error')}\")\n",
    "    print()\n",
    "\n",
    "print(f\"Success Rate: {successful_requests}/{len(results)} ({100*successful_requests/len(results):.1f}%)\")\n",
    "print(f\"Total Parallel Execution Time: {total_parallel_time:.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PERFORMANCE ANALYSIS ===\n",
      "\n",
      "Timing Statistics:\n",
      "  Average total time: 1.450s\n",
      "  Average model time: 1.429s\n",
      "  Min total time: 0.649s\n",
      "  Max total time: 2.949s\n",
      "\n",
      "Parallelization Benefits:\n",
      "  Sequential execution would take: 11.598s\n",
      "  Parallel execution took: 2.963s\n",
      "  Speedup factor: 3.91x\n",
      "\n",
      "Detailed Results Table:\n",
      " request_id  total_time  model_time                                            input_text                                        generated_text\n",
      "          1    1.145298    1.121330           translate English to German: Hello world...                                         Hallo Welt...\n",
      "          2    0.699282    0.669221          translate English to French: Good morning...                                            Bonjour...\n",
      "          3    2.039844    2.019376   translate English to Spanish: How are you today?...                                Wie sind Sie heute?...\n",
      "          4    2.479551    2.457446 summarize: The weather today is beautiful with cle... the weather today is beautiful with clear skies an...\n",
      "          5    2.949009    2.938803 summarize: Machine learning is a subset of artific... machine learning is a subset of artificial intelli...\n",
      "          6    0.655361    0.631019 question: What is the capital of France? context: ...                                              Paris...\n",
      "          7    0.980570    0.962178 question: Who invented the telephone? context: Ale...                              Alexander Graham Bell...\n",
      "          8    0.649168    0.635161         complete: The benefits of exercise include...                                          Complete:...\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrame for performance analysis\n",
    "if successful_requests > 0:\n",
    "    successful_results = [r for r in results if r['success']]\n",
    "    \n",
    "    df = pd.DataFrame(successful_results)\n",
    "    \n",
    "    print(\"=== PERFORMANCE ANALYSIS ===\\n\")\n",
    "    print(\"Timing Statistics:\")\n",
    "    print(f\"  Average total time: {df['total_time'].mean():.3f}s\")\n",
    "    print(f\"  Average model time: {df['model_time'].mean():.3f}s\")\n",
    "    print(f\"  Min total time: {df['total_time'].min():.3f}s\")\n",
    "    print(f\"  Max total time: {df['total_time'].max():.3f}s\")\n",
    "    \n",
    "    # Calculate theoretical sequential time\n",
    "    sequential_time = df['total_time'].sum()\n",
    "    speedup = sequential_time / total_parallel_time\n",
    "    \n",
    "    print(f\"\\nParallelization Benefits:\")\n",
    "    print(f\"  Sequential execution would take: {sequential_time:.3f}s\")\n",
    "    print(f\"  Parallel execution took: {total_parallel_time:.3f}s\") \n",
    "    print(f\"  Speedup factor: {speedup:.2f}x\")\n",
    "    \n",
    "    print(f\"\\nDetailed Results Table:\")\n",
    "    display_df = df[['request_id', 'total_time', 'model_time', 'input_text', 'generated_text']].copy()\n",
    "    display_df['input_text'] = display_df['input_text'].str[:50] + '...'\n",
    "    display_df['generated_text'] = display_df['generated_text'].str[:50] + '...'\n",
    "    print(display_df.to_string(index=False))\n",
    "else:\n",
    "    print(\"No successful requests to analyze\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo Summary\n",
    "\n",
    "This notebook successfully demonstrated:\n",
    "\n",
    "1. **Multiple NLP Tasks**: T5-small handled translation, summarization, question answering, and text completion\n",
    "2. **Parallel Processing**: Concurrent requests were processed efficiently by the containerized server\n",
    "3. **Performance Benefits**: Parallel execution provided significant speedup over sequential processing\n",
    "4. **Production Readiness**: The server handled multiple concurrent requests reliably\n",
    "\n",
    "## Architecture Benefits\n",
    "\n",
    "- **nginx Load Balancer**: Distributes requests across multiple workers\n",
    "- **FastAPI Async**: Non-blocking request handling for better concurrency  \n",
    "- **Containerized Deployment**: Consistent, scalable deployment environment\n",
    "- **Health Monitoring**: Kubernetes-ready health checks for production deployment\n",
    "\n",
    "The demo proves the system can handle real-world parallel inference workloads effectively."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
